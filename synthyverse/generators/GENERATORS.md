# Available Generators

|Name|Generator Object|Description|Parameters|Reference|
|:--|:--|:--|:--|:--|
|permutation|PermutationGenerator|Randomly permutes rows. Used mainly for testing.|||
|arf|ARFGenerator|Adversarial Random Forest. Uses the arfpy pypi package implementation.|**num_trees: int, default=20** <br/> Number of trees. <br/> <br/> **random_state: int, default=0** <br/> Seed for reproducibility. <br/> <br/>|[Adversarial Random Forests for Density Estimation and Generative Modeling](https://proceedings.mlr.press/v206/watson23a.html)|
|bn|BNGenerator|Bayesian Network. Uses Synthcity's implementation.|**struct_learning_n_iter: int, default=1000** <br/> Maximum number of iterations for structure learning. <br/> <br/> **struct_learning_search_method: ["hillclimb", "pc", "tree_search", "mmhc", "exhaustive"], default="tree_search"** <br/> Algorithm for learning the network structure. <br/> <br/> **struct_learning_score: ["k2", "bdeu", "bic", "bds"], default="k2"** <br/> Scoring function for structure learning. <br/> <br/> **struct_max_indegree: int, default=4** <br/> Maximum number of parents for any node. <br/> <br/> **encoder_max_clusters: int, default=10** <br/> Maximum number of clusters for continuous variable encoding. <br/> <br/> **encoder_noise_scale: float, default=0.1** <br/> Scale of noise added during encoding. <br/> <br/> **random_state: int, default=0** <br/> Seed for reproducibility. <br/> <br/>|
|ctgan|CTGANGenerator|Conditional Tabular Generative Adversarial Network. Uses ctgan pypi package implementation. This is the closest resemblance to the original paper's implementation, closer than, e.g., Synthcity.|**embedding_dim: int, default=128** <br/> Embedding dimension. <br/> <br/> **generator_dim: tuple, default=(256, 256)** <br/> Dimensions (hidden nodes) of the generator neural network. <br/> <br/> **discriminator_dim: tuple, default=(256, 256)** <br/> Dimensions (hidden nodes) of the discriminator neural network. <br/> <br/> **generator_lr: float, default=2e-4** <br/> Generator learning rate. <br/> <br/> **generator_decay: float, default=1e-6** <br/> Generator weight decay. <br/> <br/> **discriminator_lr: float, default=2e-4** <br/> Discriminator learning rate. <br/> <br/> **discriminator_decay: float, default=1e-6** <br/> Discriminator learning rate. <br/> <br/> **batch_size: int, default=500** <br/> Batch size during training. <br/> <br/> **discriminator_steps: int, default=1** <br/> Number of steps to train discriminator before updating. <br/> <br/> **log_frequency: bool, default=True** <br/> Whether to use log frequency of categorical levels in conditional sampling <br/> <br/> **verbose: bool, default=True** <br/> Verbosity. <br/> <br/> **epochs: int, default=300** <br/> Training epochs. <br/> <br/> **pac: int, default=10** <br/> Packing. <br/> <br/> **cuda: bool, default=True** <br/> Whether to use GPU. <br/> <br/>|[Modeling Tabular data using Conditional GAN](https://proceedings.mlr.press/v206/watson23a.html)|
|tvae|TVAEGenerator|Tabular Variational Auto-Encoder. Uses ctgan pypi package implementation. This is the closest resemblance to the original paper's implementation, closer than, e.g., Synthcity.|**embedding_dim: int, default=128** <br/> Embedding dimension. <br/> <br/> **compress_dims: tuple, default=(128, 128)** <br/> Dimensions of the compression layers. <br/> <br/> **decompress_dims: tuple, default=(128, 128)** <br/> Dimensions of the decompression layers. <br/> <br/> **l2scale: float, default=1e-5** <br/> L2 regularization scale. <br/> <br/> **batch_size: int, default=500** <br/> Batch size during training. <br/> <br/> **epochs: int, default=300** <br/> Training epochs. <br/> <br/> **loss_factor: int, default=2** <br/> Loss factor for reconstruction. <br/> <br/> **cuda: bool, default=True** <br/> Whether to use GPU. <br/> <br/> **verbose: bool, default=True** <br/> Verbosity. <br/> <br/>|[Modeling Tabular data using Conditional GAN](https://proceedings.mlr.press/v206/watson23a.html)|
|tabddpm|TabDDPMGenerator|Tabular diffusion model. Uses Synthcity's implementation, with minor tweaks to allow explicitly passing which columns are discrete.|TBD|[TabDDPM: Modelling Tabular Data with Diffusion Models](https://proceedings.mlr.press/v202/kotelnikov23a.html)|
|tabsyn|TabSynGenerator|Tabular Latent Diffusion Model. Uses the original paper's implementation. Tweaks were made to ensure training and generation is performed in-memory. No tweaks were made to the modelling logic.|TBD|[Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space](https://openreview.net/forum?id=4Ay23yeuz0)|
|cdtd|CDTDGenerator|Continuous Diffusion Model for Mixed-Type Tabular Data. Enforces a unified continuous noise distribution for both numerical and categorical features. Shows improved performance versus previous tabular diffusion models, e.g., TabDDPM and TabSyn. Uses the [simple CDTD wrapper](https://github.com/muellermarkus/cdtd_simple), which was published by the original paper's authors.|TBD|[Continuous Diffusion for Mixed-Type Tabular Data](https://arxiv.org/abs/2312.10431)| 
|tabargn|TabARGNGenerator|MostlyAI's tabular model. Uses masked transformers to model conditional marginal distributions. Uses the MostlyAI engine.|TBD|[TabularARGN: A Flexible and Efficient Auto-Regressive Framework for Generating High-Fidelity Synthetic Data](https://arxiv.org/abs/2501.12012)|
|realtabformer|RealTabFormerGenerator|Tokenizes tabular data to text, and fine-tunes pre-trained GPT-2 to synthesize new data.|TBD|[REaLTabFormer: Generating Realistic Relational and Tabular Data using Transformers](https://arxiv.org/abs/2302.02041)|
|ctabgan|CTABGANGenerator|Tabular GAN from the CTABGAN+ paper. Uses extensive preprocessing schemes to better capture complex feature distributions. We use the original paper's implementation. We automatically detect which features are mixed numerical-discrete, simple gaussians, etc.|TBD|[CTAB-GAN+: enhancing tabular data synthesis](https://www.frontiersin.org/journals/big-data/articles/10.3389/fdata.2023.1296508/full)|
|forestdiffusion|ForestDiffusionGenerator|Tree-based diffusion model. We use ForestDiffusion package from pypi.|TBD|[Generating and Imputing Tabular Data via Diffusion and Flow-based Gradient-Boosted Trees](https://arxiv.org/abs/2309.09968)|
|unmaskingtrees|UnmaskingTreesGenerator|Tree-based auto-regressive diffusion model. Trains per-feature GBDTs to unmask features. We use the original paper's pypi package utrees.|TBD|TBD|
